马尔科夫决策过程（Markov Decision Process，简称为MDP）从形式上描述了强化学习的环境；这个环境是完全可观测，即可以知道所需要的关于环境的所有信息；从某种角度而言，当前告知给agent的状态(state)描述环境展开的过程；几乎所有的强化学习问题都能形式化为MDPs：

- 优化控制问题，比如差分动力学中的流体运动中，主要处理连续MDPs。
- 任何部分可观测的问题都可以完全转化为MDPs，并未逃离MDP的框架。
- 增强学习中的探索-开发困境，Bandits（一个很常见的MDP形式化，广泛用于这样的场景：某些时刻拥有一些actions集合，需要采取一个action然后得到它的reward，然后结束任务）是包含一个state的MDP，是MDPs中的一个特例。

##### 马尔科夫性质

用一句话描述就是：所谓未来，只关现在，无关曾经沧海(The furure is independent of the past given the present)。

**定义**：状态$S_t$是马尔科夫的，当且仅当：
$$
\mathbb P[S_{t+1} | S_t] = \mathbb P[S_{t+1} | S_1, \dots, S_t]
$$
从这个定义可以得到：

- 状态获取了所有历史的相关信息。
- 一旦知道状态，历史就可以丢弃。因此马尔科夫过程就是拥有马尔科夫特征状态的集合。在这样的环境中，系统中所有已知的信息都可以用当前时刻特定的一个状态描述。
- 状态是未来的充分统计。

##### 状态转移矩阵

对于一个马尔科夫状态$s$及其后继状态$s'$，状态转移概率定义为：
$$
\mathcal P_{ss'} = \mathbb P[S_{t+1}=s' | S_t=s]
$$

状态转移矩阵$\mathcal P$定义了从所有状态$s$到所有其后继状态$s'$的转移矩阵。
$$
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{to}  \\
\mathcal P = \text{from}\
\begin{bmatrix}
\mathcal P_{11} & \cdots & \mathcal P_{1n} \\
\vdots & & \vdots \\
\mathcal P_{n1} & \cdots & \mathcal P_{nn}
\end{bmatrix}
$$

其中矩阵每一行和为1。

#### 1. 马尔科夫过程

马尔科夫过程是一个无记忆的随机过程，即一个具备马尔科夫特性随机状态$S_1,S_2,\dots$的序列。具体定义如下：

**定义**：马尔科夫过程（MP，也称马尔科夫链MC）是一个元组$\langle \mathcal S, \mathcal P \rangle$：

- $\mathcal S$是一个有限的状态集；

- $\mathcal P$是状态转移概率矩阵，$\mathcal P_{ss'}=\mathbb P[S_{t+1}=s' | S_t=s]$


状态空间$\mathcal S$和转移概率矩阵$\mathcal P$完整定义了系统的动态变化过程，它模拟机器人演化的过程，这个动态过程可以完全由转移概率矩阵中的状态空间来定义。此即马尔科夫过程。下面看一个用有向图表示的学生马尔科夫链：

<img src="强化学习2-马尔科夫决策过程/MP.png" width="500px" align="middle" />

每个圆环表示一种状态，出边表示转移到对应下一个状态的概率；方块表示最终状态，不过也不特殊，可以视为自环，一种一直在吸收的状态，一直围绕着自己转。

从MC取样意味着从初始状态开始得到一个状态序列，比如这里从$S_1=C_1$开始，得到$S_1,S_2, \dots,S_T$序列，如下图所示：

<img src="强化学习2-马尔科夫决策过程/MP_Samp.png" width="300px" align="middle" />

序列长度可变，每一个采样都可视为一个从马尔科夫链采样得到的随机序列，这个序列具有马尔科夫特性，以一定的概率分布在序列上进行状态转移；它是可描述的，可用类似上面的图描述，可以从任何状态来描述，以一定的转移概率跳转至其他状态。下面是学生马尔科夫链的状态转移矩阵：

<img src="强化学习2-马尔科夫决策过程/MP_Matrix.png" width="500px" align="middle" />

可以重复地从状态转移矩阵中获得样本，马尔科夫过程结束以后就可以得到序列的样本。

#### 2. 马尔科夫激励(reward)过程(MRP)

马尔科夫激励过程就是有值(Value)判断的马尔科夫过程，这些值表示相应的状态有多好；有表示从某个马尔科夫过程取样得到的特定序列累积的激励的值。具体的定义如下

**定义**：马尔科夫激励过程是元组$\langle \mathcal S, \mathcal P, \color{red}{\mathcal R}, \color{red}{\gamma} \rangle$，其中：

- $\mathcal S$是一个有限状态集；

- $\mathcal P$是状态转移概率矩阵，$\mathcal P_{ss'}=\mathbb P[S_{t+1}=s' | S_t=s]$；

- $\color{red}{\mathcal R}$是一个激励函数，$\color{red}{R_s=\mathbb E[R_{t+1} | S_t=s]}$，为$\color{red}{\text{immediate reward}}$；

- $\color{red}{\gamma}$是一个折扣(discount)因子，$\gamma \in [0,1]$。

激励函数$\mathcal R$表示从状态$s$开始，能从这个状态获得多少reward，是即时激励，也就是离开该state时刻获得的激励；然而强化学习关心的是最大化累积的rewards总量，所以在构建MRP时会将所有激励求和；而$\mathcal R$表示当前时间为$T$状态$S$的时候，在时间$T+1$时可以获得的reward。继续看学生马尔科夫过程的例子：

<img src="强化学习2-马尔科夫决策过程/MRP.png" width="500px" align="middle" />

**定义**：回报(return)$G_t$是从时间步$t$开始的总的折扣的激励：
$$
G_t = R_{t+1} + \gamma R_{t+2} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$
注意这里没有期望，因为**只是在谈论这个时刻的一个随机样本，$G$是随机的，只是来自马尔科夫激励过程(MRP)中的一个样本**，这里讨论的是序列的rewards。

- 打折因子$\gamma \in [0,1]$是未来激励的当前值。它告诉我们你是喜欢现在的reward还是未来的reward，表示对未来所能获得的reward的在乎程度。0表示只关注眼前（最大短视），1表示同等关注未来（最大远视）。
- 在$k+1$时间步获得的奖励值$\mathsf R$是$\gamma^k \mathsf R$；
- 即时奖励在延迟奖励之上：
  - $\gamma$接近0会导致短浅的评估，越接近0表示越喜欢现在的激励；
  - $\gamma$接近1会导致远视的评估，越接近1表示越不关心现在所得的激励。

使用折扣因子$\gamma$就是引入一种判断，即相比延迟的reward更倾向于短期的reward，比起之后才能得到的reward更喜欢现在的reward，这种倾向的程度由$\gamma$给出。大多数时候强化学习都会使用折扣因子，原因如下

- 主要原因是这种形式比较容易在数学上表达打折的激励；
- 避免在循环马尔科夫过程中的无限return；
- 对未来可能并未被完整描述的不确定性；即并没有一个完美的模型，只是通过一个马尔科夫过程来对环境建模，但并不是关于环境的完美模型；但可以认为已经提出了一个不错的计划，确切地知道走向未来的步骤，但并不完全相信模型所做出的决定和评估，因此选择打折（**因为当前的回报必然包含后面的回报，所以必须要将其考虑进去**）；
- 如果reward是金融的，即时rewards会比延迟rewards赚更多的利息；这里可以认为$\gamma$是利率的倒数；
- 动物/人类的行为表现出对即时奖励的偏好；测试表明人类的这种折扣偏好有类似于双曲线的折扣；
- 有时会使用无折扣的MRP（即$\gamma=1$），最简单的情况是所有的序列都会终止。在学生MP例子中，所有序列都会在某个点结束，因此由定义知所有的returns都是有限的；有一个平均回报公式，即使是在无限序列中，也能处理无折扣的MRP/MP/MDP评估。

任何依据MP定义获得的单个样本都是有限的链，不可能获得一个无限的样本，因为这是过程的定义，获得的样本会在某个点结束。即使决策过程本身包含无限循环，任何单个抽样都是有限的连接，唯一的问题是在哪一步终止。

##### 价值(value)函数

价值函数$v(s)$给出状态$s$的长期价值，这是这个领域真正感兴趣的中心，是真正关心的值。

**定义**：MRP的状态价值函数$v(s)$是从状态$s$开始的期望回报：
$$
v(s) = \mathbb E[G_t | S_t = s]
$$
注意：**$G_t|S_T=s$是随机变量，因为从状态$s$开始，可以有多种的抽样序列，每个序列都是随机的且对应一个$G(s)$的值；而它又是$G_t$处于状态$s$的一种特殊情况，$G_t$本身是MRP的一个样本**。

用期望是因为环境是随机的，处在随机的马尔科夫过程中（环境用马尔科夫过程建模）；这种状态的一步一步从一个阶段到下一阶段的演变，我们真正关心的是那些阶段的回报的期望，这才是价值，才是给定状态的好的程度。首先需要衡量它， MRP没有最大化的概念，只是说衡量获得了多少reward。继续看学生MP的例子：

以$\gamma=\frac{1}{2}$从$S_1=C_1$开始，$G_1 = R_2 + \gamma R_3 + \cdots + \gamma^{T-2}R_T$

<img src="强化学习2-马尔科夫决策过程/MRP_eg.png" width="750px" align="middle" />

**样本的return是随机的，但价值函数并非随机，而是这个随机变量的期望**。

考虑一些折扣因子不同的情况。当$\gamma = 0$时，

<img src="强化学习2-马尔科夫决策过程/MRP_eg_0.png" width="500px" align="middle" />

当$\gamma=0.9$时，则变化为

<img src="强化学习2-马尔科夫决策过程/MRP_eg_09.png" width="500px" align="middle" />

当$\gamma=1$时的情况是

<img src="强化学习2-马尔科夫决策过程/MRP_eg_1.png" width="500px" align="middle" />

##### MRP的Bellman方程

价值函数可以分解为两部分：

- 即时奖励$R_{t+1}$；

- 后继状态的折扣价值$\gamma v(S_{t+1})$
  $$
  \begin{aligned}
  v(s) &= \mathbb E[G_t | S_t = s] \\
  &= \mathbb E[R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3}+\cdots | S_t = s] \\
  &= \mathbb E[R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \cdots) | S_t = s] \\
  &= \mathbb E[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
  &= \mathbb E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]
  \end{aligned}
  $$



补充一个上式最后两步的证明：
$$
\begin{aligned}
& \mathbb E[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
=\  &\mathbb E[R_{t+1} | S_t=s] + \gamma \mathbb E[G_{t+1} | S_t = s] \\
=\ &\mathbb E[R_{t+1} | S_t=s] + \gamma \sum_{s' \in \mathcal S} \mathcal P_{ss'} \mathbb E[G_{t+1} | S_{t+1}=s' ] \\
=\ &\mathbb E[R_{t+1} | S_t=s] + \gamma \sum_{s' \in \mathcal S} \mathcal P_{ss'} v(s') \\
=\ &\mathbb E[R_{t+1} | S_t=s] + \gamma \mathbb E[v(S_{t+1})| S_t=s] \\
=\ &\mathbb E[R_{t+1} + v(S_{t+1}) | S_t = s]
\end{aligned}
$$
需要注意的是上面的$v(S_{t+1})$与$v(s)$不同，$S_{t+1}$是一个随机变量。这里索引从$t+1$开始是认为这些量都是在采取行为后发生的，而这个action之后就转移到下一个时间步了。
$$
\begin{aligned}
\color{blue}{v(s)} &= \mathbb E[R_{t+1} + \gamma v(S_{t+1}) | S_t=s] \\
&= \mathbb E[R_{t+1} | S_t=s] + \gamma \mathbb E[v(S_{t+1})  | S_t=s] \\
&= \color{blue}{\mathcal R_s + \gamma \sum_{s' \in \mathcal S} \mathcal P_{ss'} v(s')}
\end{aligned}
$$
<img src="强化学习2-马尔科夫决策过程/Bellman1.png" width="250px" align="middle" />

也就是说一个状态的价值函数等于其自身的即时奖励加上向前看一步的价值函数的期望；在学生MRP的例子中，看其$\gamma=1,s=\text{Class 3}$时的价值函数：

<img src="强化学习2-马尔科夫决策过程/Bellman2.png" width="500px" align="middle" />

Bellman方程可以使用矩阵简洁地表达出来：
$$
\begin{aligned}
v(s) = \mathcal R_s &+ \gamma \sum_{s' \in \mathcal S} \mathcal P_{ss'} v(s') \\
&\Downarrow \\
v = \mathcal R &+ \gamma \mathcal Pv
\end{aligned}
$$
其中$v​$是每条都为一个状态的列矩阵
$$
\begin{bmatrix} v(1) \\ \vdots \\ v(n) \end{bmatrix} =
\begin{bmatrix} \mathcal R_1 \\ \vdots \\ \mathcal R_n \end{bmatrix} + \gamma
\begin{bmatrix}
\mathcal P_{11} & \cdots & \mathcal P_{1n} \\
\vdots & & \vdots \\
\mathcal P_{n1} & \cdots & \mathcal P_{nn}
\end{bmatrix}
\begin{bmatrix} v(1) \\ \vdots \\ v(n) \end{bmatrix}
$$
Bellman方程是一个线性方程，可以直接求解
$$
\begin{aligned}
v &= \mathcal R + \gamma \mathcal P v \\
(I - \gamma \mathcal P) v &= \mathcal R \\
v &= (I - \gamma \mathcal P)^{-1} \mathcal R
\end{aligned}
$$
对于$n$个状态的计算复杂度是$O(n^3)$。但这样直接的解法只在比较小的MRPs上有可能性，对于庞大的MRPs，有很多迭代的方法，比如：

- 动态规划
- 蒙特卡洛评估
- 时间差分(Temporal-Difference)学习

#### 3. 马尔科夫决策过程

马尔科夫决策过程(MDP)是有决策的马尔科夫激励过程。它是一个所有状态都是马尔科夫的环境。

**定义**：马尔科夫过程是元组$\langle \mathcal S, \color{red}{\mathcal A}, \mathcal P, \mathcal R, \gamma \rangle$，其中：

- $\mathcal S$是有限状态集；
- $\color{red}{\mathcal A}$是有限行为($\color{red}{\text{action}}$)集（**一个action可对应多个状态**）；
- $\mathcal P$是状态转移概率矩阵；
- $\mathcal P_{ss'}^{\color{red}a} = \mathbb P [S_{t+1}=s' | S_t=s, A_t=\color{red}a ]$；
- $\mathcal R$是激励函数，$\mathcal R_s^{\color{red}a} = \mathbb E[R_{t+1} | S_t=s, A_t=\color{red}a]$；
- $\gamma$是折扣因子，$\gamma \in [0,1]$。

学生马尔科夫决策过程可用图表示为：

<img src="强化学习2-马尔科夫决策过程/MDP.png" width="500px" align="middle" />

##### 策略(Policies)

**定义**：策略$\pi$是给定状态时在行为上的分布。
$$
\pi(a|s) = \mathbb P[A_t=a | S_t=s]
$$

- 策略完全定义了agent的行为；

- MDP策略依赖于当前（而非历史）状态，因此只关注现在和将来能获取的rewards，而不在乎过去获得的rewards；

- 也就是说策略是固定(stationary)的，不随时间改变，因马尔科夫过程的状态可以完全地表述出接下来会发生的事情，$A_t \sim \pi(\bullet|S_t), \forall t>0$；

- 给定MDP$\mathcal M=\langle \mathcal S, \mathcal A, \mathcal P, \mathcal R, \gamma \rangle$和一个策略$\pi$；

- 则状态序列$S_1, S_2, \cdots$是马尔科夫过程$\langle \mathcal S, \mathcal P^{\pi} \rangle$；

- 状态和奖励序列$S_1,R_2,S_2,\cdots$是马尔科夫激励过程$\langle \mathcal S, \mathcal P^{\pi}, \mathcal R^{\pi},\gamma \rangle$；

- 其中
  $$
  \begin{aligned}
  \color{red}{\mathcal P_{s,s'}^{\pi}} &= \color{red}{\sum_{a \in \mathcal A} \pi (a|s) \mathcal P_{s,s'}^a} \\
  &= \sum_{a \in \mathcal A} \mathbb P[A_t=a | S_t=s] \bullet \mathbb P[S_{t+1}=s' | S_t=s, A_t=a] \\
  &= \sum_{a \in \mathcal A} \mathbb P[S_{t+1}=s', A_t=a | S_t=s] \\
  \color{red}{\mathcal R_s^{\pi}} &= \color{red}{\sum_{a \in \mathcal A} \pi(a|s) \mathcal R_s^a} \\
  &= \sum_{a \in \mathcal A} \mathbb P[A_t=a | S_t=s] \bullet \mathbb E[R_{t+1}| S_t=s, A_t=a]
  \end{aligned}
  $$








上式需注意策略$\pi$是一个概率分布，因此$\mathcal P_{s,s'}^a$和$\mathcal R_S^a$都是期望；总是能从MDP中恢复MP和MRP。

##### 价值函数

**定义**：MDP的状态-价值函数$v_{\pi}(s)$是从状态$s$开始，遵循策略$\pi$的期望回报：
$$
v_{\pi}(s) = \mathbb E_{\pi}[G_t | S_t=s]
$$
**定义**：行为-价值函数$q_{\pi}(s,a)$是从状态$s$开始，采取行为$a$，遵循策略$\pi$，所期望的回报：
$$
q_{\pi}(s,a) = \mathbb E_{\pi}[G_t | S_t=s, A_t=a]
$$
$\mathbb E_{\pi}$的下标$\pi$表示遵循策略$\pi$抽所有行为样本，也就是说$G_t$是遵循策略$\pi$所抽取的；状态价值函数表示特定状态好的程度；行为价值函数则表示在特定状态才去特定行为的好的程度。下面是折扣因子为1的学生MDP采取随机策略的状态价值函数：

<img src="强化学习2-马尔科夫决策过程/StateValue.png" width="500px" align="middle" />

##### Bellman期望方程

状态-价值函数同样可以分解为即时奖励与后继状态折扣价值的和：
$$
v_{\pi}(s) = \mathbb E_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t=s]
$$
行为-价值函数可以有类似的分解：
$$
q_{\pi}(s,a) = \mathbb E_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t=s, A_t=a]
$$
同样向前看一步，状态-价值函数有：

<img src="强化学习2-马尔科夫决策过程/Bellman3.png" width="250px" align="middle" />
$$
v_{\pi}(s) = \sum_{a \in \mathcal A} \pi(a|s)q_{\pi}(s,a)
$$
行为价值函数有：

<img src="强化学习2-马尔科夫决策过程/Bellman4.png" width="250px" align="middle" />
$$
q_{\pi}(s,a) = \mathcal R_s^a + \gamma \sum_{s' \in \mathcal S} \mathcal P_{ss'}^a v_{\pi}(s')
$$
也可以将上面两个结合起来向前看两步，对状态-价值函数有：

<img src="强化学习2-马尔科夫决策过程/Bellman5.png" width="300px" align="middle" />
$$
v_{\pi}(s) = \sum_{a \in \mathcal A} \pi(a|s) \left( \mathcal R_s^a + \gamma \sum_{s' \in \mathcal S} \mathcal P_{ss'}^a v_{\pi} (s') \right)
$$
也就是处于状态$s$时，先看可以选择的行为，依据策略$\pi$选择行为后再看跳转的状态，最后得到的是关于$v_{\pi}(s)$的公式。对行为-价值函数同样有：

<img src="强化学习2-马尔科夫决策过程/Bellman6.png" width="300px" align="middle" />
$$
q_{\pi}(s,a) = \mathcal R_s^a + \gamma \sum_{s' \in \mathcal S} \mathcal P_{ss'}^a \sum_{a' \in \mathcal A} \pi(a'|s') q_{\pi}(s',a')
$$
即在状态$s$时先依据策略$\pi$看可以跳转的状态，再选择跳转到其状态的行为。最后得到的是关于$q_{\pi}(s,a)$的公式。这样就有了两个与价值函数有关的递归形式，也就是这两个公式的形式。
$$
\begin{aligned}
v_{\pi}(s) &= \sum_{a \in \mathcal A} \pi(a|s) \left( \mathcal R_s^a + \gamma \sum_{s' \in \mathcal S} \mathcal P_{ss'}^a v_{\pi}(s') \right) \\
q_{\pi}(s,a) &= \mathcal R_s^a + \gamma \sum_{s' \in \mathcal S} \mathcal P_{ss'}^a \sum_{a' \in \mathcal A} \pi (a'|s') q_{\pi}(s',a')
\end{aligned}
$$
相应的学生MDP的例子为：

<img src="强化学习2-马尔科夫决策过程/Bellman7.png" width="500px" align="middle" />

**特别要注意的是**：

- **在MDP部分的图表中，Pub是一个action，与Study和Facebook等一样，都是一种action；而这个action可能会导致多种情况，所以用一个点表现了出来**；如果愿意，其他action也可以这样表示，但只会导致一种情况，所以没有意义；因此，也表明**一个行为有可能导致多种状况**。
- MDP与MRP要解决的问题不一样。
- MDP也是随机的。

Bellman期望方程和解也可以用诱导的MRP简洁地表示出来：
$$
\begin{aligned}
v_{\pi}(s) = \mathbb E_{\pi} [R_{t+1} | S_t=s] &+ \gamma \mathbb E[v_{\pi}(S_{t+1}) | S_t=s] \\
&\Downarrow \\
v_{\pi} = \mathcal R^{\pi} &+ \gamma \mathcal P^{\pi} v_{\pi} \\
&\Downarrow\\
v_{\pi} = (I &- \gamma \mathcal P^{\pi})^{-1} \mathcal R^{\pi}
\end{aligned}
$$

##### 最优价值函数

先看两个定义：

- 最优状态-价值函数$v_*(s)$是所有策略中最大的价值函数，
  $$
  v_*(s) = \max_{\pi} v_{\pi}(s)
  $$

- 最优的行为-价值函数$q_*(s,a)$是所有策略中最大的行为价值函数，
  $$
  q_*(s,a) = \max_{\pi} q_{\pi}(s,a)
  $$

- 最优价值函数指定了在MDP中可能的最好的表现。

- 当得到最优价值函数后，MDP就被“解决”了。

关于学生MDP的例子，下面是最优状态-价值函数：

<img src="强化学习2-马尔科夫决策过程/Optimal_v.png" width="500px" align="middle" />

下面是最优行为-价值函数，通过从后向前求解，可以得到实际采取的行为：

- 从最后非终结状态开始（可能有多个），查看其每个行为后得到的激励，将最大的激励作为这个状态的价值；
- 递归地向前看前面的状态，选择不同的行为求解激励，将最大的作为当前状态的价值，直至第一个状态。

<img src="强化学习2-马尔科夫决策过程/Optimal_a.png" width="500px" align="middle" />

##### 最优策略

定义一个关于策略的偏序：
$$
\pi \ge \pi'\ \ \ \ 若对\forall s,\  v_{\pi}(s) \ge v_{\pi'}(s)
$$
**定理**：对任意马尔科夫决策过程，都有

- 存在优于或等于所有其他策略的最优策略$v_*$，即$\pi_* \ge \pi, \forall \pi$；
- 所有的最优策略都获得最优价值函数，即$v_{\pi_*}(s) = v_*(s)$；
- 所有的最优策略都获得最优行为-价值函数，即$q_{\pi_*}(s,a) = q_*(s,a)$。

最优策略可以通过在$q_*(s,a)$上最大化获得：
$$
\pi_*(a,s) = 
\begin{cases}
1 & \text{若}a=\text{argmax}_{a \in \mathcal A} q_*(s,a) \\
0 & \text{otherwise}
\end{cases}
$$

- 对任意MDP都存在确定性的最优策略；
- 如果得到$q_*(s,a)$，则立刻就获得了最优策略

求解学生MDP的最优策略为，用红线标出了每个状态的最优策略：

<img src="强化学习2-马尔科夫决策过程/Optimal_p.png" width="500px" align="middle" />

是否最优策略就是定义在每个状态的行为选择？

最优价值函数通过Bellman最优性方程递归地关联：

<img src="强化学习2-马尔科夫决策过程/BellmanOptimality.png" width="250px" align="middle" />
$$
v_*(s) = \max_a q_*(s,a)
$$
即最优的状态-价值函数为最优行为-价值函数所有行为中的最大值。同样也可以由最优状态-价值函数得到最优行为-价值函数：

<img src="强化学习2-马尔科夫决策过程/BellmanOptimality2.png" width="250px" align="middle" />
$$
q_*(s,a) = \mathcal R_S^a + \gamma \sum_{s' \in \mathcal S} \mathcal P_{ss'}^a v_*(s')
$$
将两者结合起来，就可以得到状态-价值函数递归的形式：

<img src="强化学习2-马尔科夫决策过程/BellmanOptimality3.png" width="300px" align="middle" />
$$
v_*(s) = \max_a \mathcal R_s^a + \gamma \sum_{s' \in \mathcal S} \mathcal P_{ss'}^a v_*(s')
$$
同样也可以得到行为-价值函数的递归形式：

<img src="强化学习2-马尔科夫决策过程/BellmanOptimality4.png" width="300px" align="middle" />
$$
q_*(s,a) = \mathcal R_s^a + \gamma \sum_{s' \in \mathcal S} \mathcal P_{ss'}^a \max_{a'} q_*(s',a')
$$
关于学生MDP的情况就是：

<img src="强化学习2-马尔科夫决策过程/BellmanOptimality5.png" width="500px" align="middle" />

- Bellman最优性方程是非线性的
- 通常不存在显式的公式
- 有很多迭代的方法
  - 值迭代
  - 策略迭代
  - Q学习
  - Sarsa

#### 4. MDPs扩展

关于如何解决概率随时间变化的问题有两个方法：一个是可以有一个不稳定(Non-Stationary)马尔科夫过程，使用stationary MDP的算法，但逐渐调整自己的解决算法来找到最好的解决方案； 另一个是已有不稳定的动态过程，可以让它变成更加复杂的马尔科夫过程，比如假设状态转移的概率依赖于在这个状态上停留的时间，这样可以增加它们的状态概率，就构造了一个更复杂的马尔科夫过程，但不改变马尔科夫过程的基本结构。

对MDPs的扩展有：

- 无限与连续MDPs；
- 部分可观测MDPs；
- 无折扣，均值激励MDPs。

##### 无限MDPs

下面所有的扩展都是可能的：

- 无限可数状态和／或行为空间
  - 直接求解；
- 连续状态和／或行为空间
  - 接近于线性二次模型的形式(LQR)
- 连续时间
  - 要求偏微分方程
  - Hamilton-Jacobi-Bellman(HJB)方程
  - Bellman方程当时间步趋于0时的极限形式

##### POMDPs

部分可观测马尔科夫决策过程(POMDP)是有隐状态的MDP。是有行为的隐马尔科夫模型。

**定义**：POMDP是元组$\langle \mathcal S, \mathcal A, \color{red}{\mathcal O}, \mathcal P, \mathcal R, \color{red}{\mathcal Z}, \gamma\rangle$：

- $\mathcal S$是有限状态集；
- $\mathcal A$是有限行为集；
- $\color{red}{\mathcal O}$是有限观测集；
- $\mathcal P$是状态转移概率矩阵，$\mathcal P_{ss'}^a = \mathbb P[S_{t+1}=s'; S_t=s, A_t=a]$；
- $\mathcal R$是激励函数，$\mathcal R_s^a=\mathbb E[R_{t+1} | S_t=s, A_t=a]$；
- $\mathcal Z$是观察函数，$\mathcal Z_{s'o}^a = \mathbb P[O_{t+1}=o | S_{t+1}=s', A_t=a]$；
- $\gamma$是折扣因子，$\gamma \in [0,1]$。

信念状态：

**定义**：历史$H_t$是一系列的行为，观察和激励：
$$
H_t = A_0, O_1, R_1, \cdots, A_{t-1}, O_t, R_t
$$
**定义**：信念状态$b(h)$是关于状态的以历史$h$为条件的概率分布：
$$
b(h) = (\mathbb P[S_t=s^1 | H_t=h], \cdots,\mathbb P[S_t=s^n H_t=h])
$$
POMDPs的规约：

- 历史$H_t$满足马尔科夫特性；

- 信念状态$b(H_t)$满足马尔科夫特性；

  <img src="强化学习2-马尔科夫决策过程/POMDP.png" width="750px" text-align="middle" />

- POMDP可归约为一个（无限）历史树；

- POMDP可归约为一个（无限）信念状态树。

##### 遍历马尔科夫过程

一个遍历马尔科夫过程是：

- *循环的*：每个状态都会无限次被访问；
- *非周期的*：每个状态的访问并没有系统周期。

**定理**：遍历马尔科夫过程有含如下特性的有限静止分布$d^{\pi}(s)$：
$$
d^{\pi}(s) = \sum_{s' \in \mathcal S} d^{\pi}(s')\mathcal P_{s's}
$$
**定义**：若马尔科夫链被任意遍历策略包含，则此MDP是遍历的。

对任何策略$\pi$，遍历MDP每一个时间步都有一个与初始状态无关的平均激励$\rho^{\pi}$：
$$
\rho^{\pi} = \lim_{T \to \infty} \frac{1}{T} \mathbb E \left[ \sum_{t=1}^T R_t \right]
$$
平均激励价值函数：

- 一个无折扣遍历MDP的价值函数可用平均激励表达；

- $\tilde{v}_{\pi}(s)$是因从状态$s$开始的额外激励：
  $$
  \tilde{v}_{\pi}(s) = \mathbb E_{\pi} \left[ \sum_{k=1}^{\infty} (R_{t+k}) - \rho^{\pi} |S_t=s \right]
  $$




也有相应的平均激励Bellman方程：
$$
\begin{aligned}
\tilde{v}_{\pi}(s)
&= \mathbb E_{\pi} \left[ (R_{t+1}-\rho^{\pi}) +\sum_{k=1}^\infty(R_{t+k+1} - \rho^{\pi}) | S_t=s \right] \\
&= \mathbb E_{\pi} \left[ (R_{t+1} - \rho^{\pi}) + \tilde{v}_{\pi}(S_{t+1}) | S_t=s \right]
\end{aligned}
$$
