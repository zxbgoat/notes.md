图是一种对一系列物体（节点）和关系（边）进行建模的非欧数据结构，图分析主要关注节点分类、链接预测和聚类，图神经网络（Graph Neural Networks）是基于深度学习的在图领域进行操作的方法。

GNN的首个推动源于卷积神经网络，CNN能构提取多尺度局部空间特征并将其组合起来构造高表达能力特征的能力，随着对CNN和图的深入研究，我们发现CNN的关键：局部连接、权值共享、使用多层，在图领域也十分重要：因1）图是最典型的局部连接的结构；2）全职共享相比传统谱图理论减少了计算代价；3）多层结构是处理层级模式的关键，能获取不同尺寸的特征。但CNN仅能处理图像（2D网格）和文本（1D序列）这样的普通欧式数据，尽管可以将它们看成是图的个例。

另一个推动则来自图嵌入，它会学习使用低维向量来表示图节点、边和子图。遵循表示学习的思想和词嵌入的成功，首个基于表示学习的图嵌入方法DeepWalk将跳元模型应用在生成的随机游走上，类似的方法如node2vec、LINE和TADW也获得了突破。但这些方法有两个缺点：一是编码器中没有节点共享参数使得计算效率不高，二是直接的嵌入方法缺乏泛化能力。

基于CNN和嵌入，研究者提出图神经网络来从图结构共同地集合信息，因此能够对元素及其依赖组成的输入和/或输出进行建模，甚至能够使用RNN核同时对图上的扩散过程进行建模。

下面解释图神经网络值得被研究的几个基本原因：

- 第一，CNN和RNN这样标准的神经网络无法恰当地处理图输入，因其会将节点特征按照特定顺序堆叠，但图中并没有一个自然的顺序，要完整地表达一个图，需要遍历所有的可能顺序来作为CNN和RNN模型的输入，这在计算时是十分冗余的。为解决这个问题，GNN忽视节点输入顺序，在每个节点上分别传播，换句话说GNN的输出与节点输入顺序无关。
- 第二，图中的边表达了两个节点之间的依赖关系，在标准神经网络中，依赖信息仅作为节点特征考虑，但GNN能够执行图特征引导的传播，而非仅仅特征的一部分。GNN通常使用其邻接节点状态的加权和来更新隐状态。
- 第三，在高级人工智能研究中推理是非常重要的研究课题，而人类大脑的推理几乎都是基于日常经验提取的图上的；GNN能够从场景图像和故事文档这样的非结构数据中探索生成图，而这会是高级AI进一步研究的有力神经模型。最近已经证明一个简单结构的未训练GNN也能表现得很好。

本文提供了不同GNN模型的彻底回顾及其应用的系统分类。

#### 模型

我们首先描述原始的图神经网络工作，也列出其在表达能力和训练效率上的局限；然后引入几种旨在缓解这些局限的GNN变体，这些变体在不同类型的图上执行，使用不同的传播函数和高级训练方法；之后展示三种能够推广和扩展到许多工作的通用框架，其中消息传播网络（MPNN）统一了多种图神经网络和图卷积网络，非局部网络（NLNN）统一了几种自注意力风格的方法，以及图网络（GN）能够推广到几乎所有本文提到的GNN变体。在此之前先引入一些记号：

|                          记号                           |                 描述                  |
| :-----------------------------------------------------: | :-----------------------------------: |
|                      $\mathbb R^m$                      |             $m$维欧氏空间             |
|                 $a,\mathbf a,\mathbf A$                 |           标量、向量、矩阵            |
|                      $\mathbf A^T$                      |               矩阵转置                |
|                      $\mathbf I_N$                      |             $N$维单位矩阵             |
|           $\mathbf g_\theta \star \mathbf x$            | $\mathbf g_\theta$和$\mathbf x$的卷积 |
|                           $N$                           |               节点个数                |
|                          $N^v$                          |               节点个数                |
|                          $N^e$                          |                边个数                 |
|                     $\mathcal N_v$                      |           节点$v$的近邻集合           |
|                     $\mathbf a_v^t$                     |   节点$v$在时间$t$的向量$\mathbf a$   |
|                      $\mathbf h_v$                      |            节点$v$的隐状态            |
|                     $\mathbf h_v^t$                     |       节点$v$在时间$t$的隐状态        |
|                    $\mathbf e_{vw}$                     |         节点$v$到$w$边的特征          |
|                      $\mathbf e_k$                      |          标记为$k$的边的特征          |
|                     $\mathbf o_v^t$                     |             节点$v$的输出             |
| $\mathbf W^i,\mathbf U^i,\mathbf W^o,\mathbf U^o,\dots$ | 计算$\mathbf i,\mathbf o,\dots$的矩阵 |
|               $\mathbf b^i, \mathbf b^o$                | 计算$\mathbf i,\mathbf o,\dots$的向量 |
|                        $\sigma$                         |              sigmoid函数              |
|                         $\rho$                          |           可供选非线形函数            |
|                         $\tanh$                         |             双曲正切函数              |
|                   $\text{LeakyReLU}$                    |             LeakyReLU函数             |
|                                                         |            逐元素相乘操作             |
|                         $\Vert$                         |               向量拼接                |

##### 图神经网络

图神经网络首先在[文献1]()中被提出，它扩展已有的神经网络来处理以图领域表达的数据。在一个图中，每个节点都很自然地用其特征和相关节点来表达。GNN的目标是学习一个包含每个节点近邻信息的状态嵌入$\mathbf h_v \in \mathbb R^s$，$\mathbf h_v$是节点$v$的一个$s$维向量，能用于产生作为标签的输出$\mathbf o_v$。称参数化函数$f$为**局部转移函数**，它被所有节点共享并按照近邻的输入更新节点状态；令$g$为**局部输出函数**，它描述产生输出的方式。这样，$\mathbf h_v$和$\mathbf o_v$就定义为：
$$
\begin{eqnarray}
\mathbf h_v &=& f\left( \mathbf x_v, \mathbf x_{co[v]}, \mathbf h_{ne[v]}, \mathbf x_{ne[v]} \right)\tag{1}\\
\mathbf o_v &=& g\left( \mathbf h_v, \mathbf x_v \right) \tag{2}
\end{eqnarray}
$$
其中$\mathbf x_v$是$v$的特征，$\mathbf x_{co[v]}$是其边的特征，$\mathbf h_{ne[v]}$、$\mathbf x_{ne[v]}$是节点近邻的状态和特征。令$\mathbf H$、$\mathbf O$、$\mathbf X$和$\mathbf X_N$分别为堆叠所有状态、输出、特征和节点特征所构造的向量，则就有一个紧致的形式：
$$
\begin{eqnarray}
\mathbf H &=& F(\mathbf H, \mathbf X) \tag{3}\\
\mathbf O &=& G(\mathbf H, \mathbf X_N) \tag{4}
\end{eqnarray}
$$
其中**全局转移函数**$F$，**全局输出函数**$G$分别是所有节点的$f$和$g$的堆叠版。

